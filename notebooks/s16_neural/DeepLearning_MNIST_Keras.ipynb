{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 150px\" src=\"https://sc.acamica.com/icons/1j7w9h/social-300x300.png\">  Acámica DS-COR3 - Ejercicio de Redes Neuronales\n",
    "\n",
    "### Junio 2019\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción\n",
    "\n",
    "El objetivo de este notebook es ejercitar el ajuste de una Red Neuronal básica sobre un conjunto de datos de dígitos manuscritos de MNIST. En este caso se utilizará Keras como librería para modelar un perceptrón multicapa (MLP), que además provee el dataset a ser utilizado. El notebook está basado en un [ejemplo provisto por Keras](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py), por lo cual el ejercicio es seguir paso a paso el desarrollo del mismo y probar ciertos cambios propuestos por los items marcados como **\\*\\*PREGUNTAS\\*\\***, así como documentar el efecto de aplicar esos cambios. \n",
    "\n",
    "**Recursos**:  \n",
    "- https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py\n",
    "- https://medium.com/@mjbhobe/mnist-digits-classification-with-keras-ed6c2374bd0e\n",
    "- https://www.kaggle.com/adityaecdrid/mnist-with-keras-for-beginners-99457\n",
    "- https://towardsdatascience.com/deep-learning-tips-and-tricks-1ef708ec5f53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Dependencias\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 train samples\n",
      "10000 valid samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Cargar datasets de MNIST, en conjuntos de train y test\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Flatten de matrices de 28x28 a vectores de 784 elementos\n",
    "x_train = x_train.reshape(len(x_train), 784)\n",
    "x_test = x_test.reshape(len(x_test), 784)\n",
    "\n",
    "# Casting de int a float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Min Max scaling: Rango [0, 255] -> [0, 1]\n",
    "# **PREGUNTA**: Qué efecto tiene en el entrenamiento no hacerlo? \n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Por último, separar un conj de validación para monitoreo\n",
    "# Dado que ya está shuffled, se puede tomar los primeros N\n",
    "x_valid = x_train[-10000:, :]\n",
    "y_valid = y_train[-10000:]\n",
    "x_train = x_train[:-10000, :]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_valid.shape[0], 'valid samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# Convertir etiquetas: vector de 10 categorías en matriz binaria de 10 elementos\n",
    "# De esta forma, ahora el modelo provee como salida una probabilidad por cada clase\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_valid = to_categorical(y_valid, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leferrad/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/leferrad/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Primero se define la ARQUITECTURA\n",
    "# **PREGUNTAS:**\n",
    "# 1. Qué pasa si aumento / reduzco la cantidad de neuronas en la primer capa?\n",
    "# 2. Qué efecto tiene agregar 2 capas de 512 neuronas?\n",
    "# 3. Qué ocurre si elimino las capas de DropOut?\n",
    "# 4. Qué efecto tiene cambiar las activaciones de 'relu' a 'tanh'\n",
    "# 5. Probar agregar una capa de BatchNormalization luego de la primer capa oculta. Qué ocurre y por qué?\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luego definimos la OPTIMIZACIÓN\n",
    "# **PREGUNTAS:**\n",
    "# 1. Qué efecto tiene usar un optimizador de tasa fija como SGD\n",
    "# 2. Qué ocurre si se aumenta la tasa de aprendizaje (parámetro 'lr')\n",
    "# 3. Qué pasa si uso un batch_size de 10? y uno de 500?\n",
    "# 4. Analizar la evolución del loss en train y valid, en términos de las épocas (probar hasta 100) \n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leferrad/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.2626 - acc: 0.9191 - val_loss: 0.1316 - val_acc: 0.9608\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.1089 - acc: 0.9662 - val_loss: 0.0935 - val_acc: 0.9721\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 0.0788 - acc: 0.9764 - val_loss: 0.1041 - val_acc: 0.9702\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 0.0611 - acc: 0.9820 - val_loss: 0.0796 - val_acc: 0.9776\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 0.0534 - acc: 0.9838 - val_loss: 0.0846 - val_acc: 0.9783\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 0.0449 - acc: 0.9862 - val_loss: 0.0946 - val_acc: 0.9787\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 0.0382 - acc: 0.9887 - val_loss: 0.0919 - val_acc: 0.9800\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 0.0338 - acc: 0.9900 - val_loss: 0.1003 - val_acc: 0.9800\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.0315 - acc: 0.9905 - val_loss: 0.1084 - val_acc: 0.9777\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.0276 - acc: 0.9919 - val_loss: 0.0977 - val_acc: 0.9809\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.0239 - acc: 0.9935 - val_loss: 0.1018 - val_acc: 0.9809\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 0.0259 - acc: 0.9926 - val_loss: 0.1066 - val_acc: 0.9797\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.0214 - acc: 0.9937 - val_loss: 0.1103 - val_acc: 0.9808\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.0190 - acc: 0.9941 - val_loss: 0.1209 - val_acc: 0.9797\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.0197 - acc: 0.9946 - val_loss: 0.1308 - val_acc: 0.9795\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.0178 - acc: 0.9950 - val_loss: 0.1133 - val_acc: 0.9819\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 0.0183 - acc: 0.9950 - val_loss: 0.1307 - val_acc: 0.9808\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 0.0187 - acc: 0.9949 - val_loss: 0.1388 - val_acc: 0.9795\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 0.0177 - acc: 0.9951 - val_loss: 0.1345 - val_acc: 0.9805\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 0.0171 - acc: 0.9953 - val_loss: 0.1255 - val_acc: 0.9811\n"
     ]
    }
   ],
   "source": [
    "# Ajustar el modelo!\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.12511385221116994\n",
      "Test accuracy: 0.9824\n"
     ]
    }
   ],
   "source": [
    "# Finalmente evaluamos los resultados en test\n",
    "# **PREGUNTAS**\n",
    "# 1. Puede haber inconveniente de utilizar \"accuracy\" para interpretar el desempeño? \n",
    "#    (Pensar en cant de clases, desbalance en los datos, etc)\n",
    "# 2. Calcular precision, recall y f1-score sobre test e interpretar resultados\n",
    "# 3. Cómo se compara el loss en test con el de train? Qué podría indicar?\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
